#!/usr/bin/env python3
"""
Sandhi Rules Analysis
====================

This script analyzes how tokens are generated based on Tamil sandhi rules
and verifies the reverse sandhi implementation in sandhi.py.
"""

import sys
import os

# Add GPE directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'GPE'))

def analyze_sandhi_rules():
    """Analyze the sandhi rules and their application."""
    print("=" * 80)
    print("TAMIL SANDHI RULES ANALYSIS")
    print("=" * 80)
    
    try:
        from sandhi import (
            TA_RULES, sandhi_mark, sandhi_split, remove_boundaries, 
            apply_rules, LANG_RULES, BOUND
        )
        print("‚úÖ Successfully imported sandhi functions")
    except ImportError as e:
        print(f"‚ùå Failed to import sandhi functions: {e}")
        return False
    
    print(f"\nüìä RULE STATISTICS:")
    print(f"Total Tamil Sandhi Rules: {len(TA_RULES)}")
    print(f"Boundary Symbol: '{BOUND}' (Unicode: U+{ord(BOUND):04X})")
    
    return True

def test_sandhi_rule_categories():
    """Test different categories of sandhi rules."""
    print("\n" + "=" * 80)
    print("SANDHI RULE CATEGORIES TESTING")
    print("=" * 80)
    
    try:
        from sandhi import sandhi_mark, sandhi_split, TA_RULES, BOUND
    except ImportError as e:
        print(f"‚ùå Import failed: {e}")
        return False
    
    # Test cases for different rule categories
    test_cases = {
        "A) Vowel-Vowel Combinations": [
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡ÆÖ‡Æ®‡Øç‡Æ§",  # ‡ÆÖ + ‡ÆÖ
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡ÆÜ",     # ‡ÆÖ + ‡ÆÜ  
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æá",     # ‡ÆÖ + ‡Æá
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æâ",     # ‡ÆÖ + ‡Æâ
            "‡Æá‡Æµ‡Æ©‡Øç ‡ÆÖ",     # ‡Æá + ‡ÆÖ
            "‡Æâ‡Æµ‡Æ©‡Øç ‡ÆÖ",     # ‡Æâ + ‡ÆÖ
        ],
        
        "B) Glide Insertion Cues": [
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡ÆØ",     # ‡Æá‡Æü‡Øà‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æµ",     # ‡Æá‡Æü‡Øà‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ
            "‡Æá‡Æµ‡Æ©‡Øç ‡ÆØ",     # ‡Æá + ‡ÆØ
            "‡Æâ‡Æµ‡Æ©‡Øç ‡Æµ",     # ‡Æâ + ‡Æµ
        ],
        
        "C) Nasal + Stop Assimilations": [
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æï",     # ‡Æô‡Øç + ‡Æï
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æö",     # ‡Æû‡Øç + ‡Æö
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æü",     # ‡Æ£‡Øç + ‡Æü
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ§",     # ‡Æ®‡Øç + ‡Æ§
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ™",     # ‡ÆÆ‡Øç + ‡Æ™
        ],
        
        "D) Gemination/Doubling": [
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æï‡Øç‡Æï‡ØÅ",   # ‡Æï‡Øç + ‡Æï
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æö‡Øç‡Æö",    # ‡Æö‡Øç + ‡Æö
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æü‡Øç‡Æü",    # ‡Æü‡Øç + ‡Æü
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ§‡Øç‡Æ§",    # ‡Æ§‡Øç + ‡Æ§
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ™‡Øç‡Æ™",    # ‡Æ™‡Øç + ‡Æ™
        ],
        
        "E) Mutation Cues (‡Æ§‡Æø‡Æ∞‡Æø‡Æ§‡Æ≤‡Øç)": [
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æö",     # ‡Æ≤‡Øç + ‡Æö
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ∞",     # ‡Æ∞‡Øç + ‡Æ∞
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æü",     # ‡Æ©‡Øç + ‡Æü
        ],
        
        "F) Final Consonant Loss": [
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡ÆÖ",     # ‡Æï‡Øç + ‡ÆÖ
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æá",     # ‡Æö‡Øç + ‡Æá
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æâ",     # ‡Æü‡Øç + ‡Æâ
        ],
        
        "G) Case Suffix & Postpositions": [
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æê",     # ‡Æµ‡Æø‡Æ©‡Øà‡Æö‡Øç‡Æö‡Øä‡Æ≤‡Øç + ‡Æê
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æï‡Øç‡Æï‡ØÅ",   # ‡Æµ‡Æø‡Æ©‡Øà‡Æö‡Øç‡Æö‡Øä‡Æ≤‡Øç + ‡Æï‡Øç‡Æï‡ØÅ
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡ÆÜ‡Æ≤‡Øç",   # ‡Æµ‡Æø‡Æ©‡Øà‡Æö‡Øç‡Æö‡Øä‡Æ≤‡Øç + ‡ÆÜ‡Æ≤‡Øç
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æï‡Æ≥‡Øç",   # ‡Æµ‡Æø‡Æ©‡Øà‡Æö‡Øç‡Æö‡Øä‡Æ≤‡Øç + ‡Æï‡Æ≥‡Øç
        ],
        
        "H) Verbal Participle Joins": [
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ™‡Øã",    # ‡Æá + ‡Æ™‡Øã
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æµ‡Ææ",    # ‡Æâ + ‡Æµ‡Ææ
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æá‡Æ∞‡ØÅ",   # ‡ÆÖ + ‡Æá‡Æ∞‡ØÅ
        ],
        
        "I) Numeral + Classifier": [
            "123 ‡ÆÜ‡ÆÆ‡Øç",    # Number + ‡ÆÜ‡ÆÆ‡Øç
            "456 ‡Æê",      # Number + ‡Æê
        ],
        
        "J) Generic Whitespace Suppression": [
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ",  # Word + Word
            "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æö‡ØÜ‡Æ©‡Øç‡Æ±‡Ææ‡Æ©‡Øç",  # Word + Word
        ]
    }
    
    for category, test_texts in test_cases.items():
        print(f"\n{category}:")
        print("-" * 60)
        
        for text in test_texts:
            # Apply sandhi marking
            marked = sandhi_mark(text, lang="ta")
            
            # Split into tokens
            tokens = sandhi_split(text, lang="ta")
            token_texts = [token for token, _ in tokens]
            
            # Check if boundaries were added
            has_boundaries = BOUND in marked
            boundary_count = marked.count(BOUND)
            
            print(f"  Input: '{text}'")
            print(f"  Marked: '{marked}'")
            print(f"  Tokens: {token_texts}")
            print(f"  Boundaries: {boundary_count} ({'‚úÖ' if has_boundaries else '‚ùå'})")
            print()
    
    return True

def test_token_generation_process():
    """Test the complete token generation process."""
    print("\n" + "=" * 80)
    print("TOKEN GENERATION PROCESS ANALYSIS")
    print("=" * 80)
    
    try:
        from sandhi import sandhi_mark, sandhi_split, apply_rules, TA_RULES
    except ImportError as e:
        print(f"‚ùå Import failed: {e}")
        return False
    
    # Test with complex Tamil text
    test_texts = [
        "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡ØÜ‡Æ©‡Øç‡Æ±‡Ææ‡Æ©‡Øç",
        "‡ÆÖ‡Æµ‡Æ≥‡Øç ‡Æ™‡ØÅ‡Æ§‡Øç‡Æ§‡Æï‡ÆÆ‡Øç ‡Æ™‡Æü‡Æø‡Æ§‡Øç‡Æ§‡Ææ‡Æ≥‡Øç", 
        "‡Æ®‡Ææ‡Æ©‡Øç ‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ±‡Øç‡Æï‡ØÅ ‡Æ™‡Øã‡Æ©‡Øá‡Æ©‡Øç",
        "‡ÆÖ‡Æµ‡Æ©‡Øç ‡ÆÖ‡Æ®‡Øç‡Æ§ ‡ÆÖ‡Æ™‡Øç‡Æ™‡Æ§‡Øç‡Æ§‡Øà ‡Æâ‡Æ£‡Øç‡Æü‡ØÅ",
        "‡Æ®‡Ææ‡Æ©‡Øç Hello world ‡Æ™‡Æ£‡Øç‡Æ£‡Æø‡Æü‡Øç‡Æü‡Øá‡Æ©‡Øç",  # Code-mixed
    ]
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nTest {i}: {text}")
        print("-" * 50)
        
        # Step 1: Apply sandhi rules
        marked = sandhi_mark(text, lang="ta")
        print(f"Step 1 - Sandhi Marking:")
        print(f"  Original: '{text}'")
        print(f"  Marked:   '{marked}'")
        print(f"  Boundaries added: {marked.count('‚üÇ')}")
        
        # Step 2: Split into tokens
        tokens = sandhi_split(text, lang="ta")
        token_texts = [token for token, _ in tokens]
        print(f"Step 2 - Token Splitting:")
        print(f"  Tokens: {token_texts}")
        print(f"  Token count: {len(tokens)}")
        
        # Step 3: Verify token boundaries
        print(f"Step 3 - Token Boundary Analysis:")
        for j, (token, (start, end)) in enumerate(tokens):
            print(f"  Token {j+1}: '{token}' (positions {start}-{end})")
        
        print()
    
    return True

def test_reverse_sandhi_verification():
    """Verify reverse sandhi implementation."""
    print("\n" + "=" * 80)
    print("REVERSE SANDHI VERIFICATION")
    print("=" * 80)
    
    try:
        from sandhi import sandhi_mark, sandhi_split, remove_boundaries
    except ImportError as e:
        print(f"‚ùå Import failed: {e}")
        return False
    
    test_cases = [
        "‡ÆÖ‡Æµ‡Æ©‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡ØÜ‡Æ©‡Øç‡Æ±‡Ææ‡Æ©‡Øç",
        "‡ÆÖ‡Æµ‡Æ≥‡Øç ‡Æ™‡ØÅ‡Æ§‡Øç‡Æ§‡Æï‡ÆÆ‡Øç ‡Æ™‡Æü‡Æø‡Æ§‡Øç‡Æ§‡Ææ‡Æ≥‡Øç",
        "‡Æ®‡Ææ‡Æ©‡Øç ‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Æø‡Æ±‡Øç‡Æï‡ØÅ ‡Æ™‡Øã‡Æ©‡Øá‡Æ©‡Øç",
        "‡ÆÖ‡Æµ‡Æ©‡Øç ‡ÆÖ‡Æ®‡Øç‡Æ§ ‡ÆÖ‡Æ™‡Øç‡Æ™‡Æ§‡Øç‡Æ§‡Øà ‡Æâ‡Æ£‡Øç‡Æü‡ØÅ",
    ]
    
    print("Testing reverse sandhi (encode ‚Üí decode):")
    print("-" * 50)
    
    for i, original_text in enumerate(test_cases, 1):
        print(f"\nTest {i}: {original_text}")
        
        # ENCODING PHASE
        # Step 1: Apply sandhi marking
        marked_text = sandhi_mark(original_text, lang="ta")
        print(f"  Encoding:")
        print(f"    Original: '{original_text}'")
        print(f"    Marked:   '{marked_text}'")
        
        # Step 2: Split into tokens
        tokens = sandhi_split(original_text, lang="ta")
        token_texts = [token for token, _ in tokens]
        print(f"    Tokens:   {token_texts}")
        
        # DECODING PHASE
        # Step 3: Reconstruct by joining tokens
        reconstructed_from_tokens = "".join(token_texts)
        print(f"  Decoding:")
        print(f"    From tokens: '{reconstructed_from_tokens}'")
        
        # Step 4: Remove boundaries from marked text
        reconstructed_from_boundaries = remove_boundaries(marked_text)
        print(f"    From boundaries: '{reconstructed_from_boundaries}'")
        
        # VERIFICATION
        tokens_match = reconstructed_from_tokens == reconstructed_from_boundaries
        content_preserved = len(reconstructed_from_tokens) > 0
        
        print(f"  Verification:")
        print(f"    Tokens match boundaries: {'‚úÖ' if tokens_match else '‚ùå'}")
        print(f"    Content preserved: {'‚úÖ' if content_preserved else '‚ùå'}")
        
        if not tokens_match:
            print(f"    ‚ö†Ô∏è  Mismatch detected!")
    
    return True

def analyze_rule_application_order():
    """Analyze the order of rule application."""
    print("\n" + "=" * 80)
    print("RULE APPLICATION ORDER ANALYSIS")
    print("=" * 80)
    
    try:
        from sandhi import TA_RULES, apply_rules
    except ImportError as e:
        print(f"‚ùå Import failed: {e}")
        return False
    
    print("Sandhi Rules by Category:")
    print("-" * 50)
    
    categories = {
        "Vowel-Vowel (A)": TA_RULES[:10],  # First 10 rules
        "Glide Insertion (B)": TA_RULES[10:20],  # Next 10 rules
        "Nasal+Stop (C)": TA_RULES[20:30],  # Next 10 rules
        "Gemination (D)": TA_RULES[30:40],  # Next 10 rules
        "Mutation (E)": TA_RULES[40:50],  # Next 10 rules
        "Final Loss (F)": TA_RULES[50:60],  # Next 10 rules
        "Case Suffix (G)": TA_RULES[60:70],  # Next 10 rules
        "Verbal (H)": TA_RULES[70:80],  # Next 10 rules
        "Numeral (I)": TA_RULES[80:90],  # Next 10 rules
        "Generic (J)": TA_RULES[90:],  # Remaining rules
    }
    
    for category, rules in categories.items():
        if rules:
            print(f"\n{category}: {len(rules)} rules")
            for i, rule in enumerate(rules[:3]):  # Show first 3 rules
                pattern_str = rule.pattern.pattern[:50] + "..." if len(rule.pattern.pattern) > 50 else rule.pattern.pattern
                print(f"  Rule {i+1}: {pattern_str}")
            if len(rules) > 3:
                print(f"  ... and {len(rules)-3} more rules")
    
    return True

def test_edge_cases():
    """Test edge cases in sandhi rule application."""
    print("\n" + "=" * 80)
    print("EDGE CASES TESTING")
    print("=" * 80)
    
    try:
        from sandhi import sandhi_mark, sandhi_split
    except ImportError as e:
        print(f"‚ùå Import failed: {e}")
        return False
    
    edge_cases = [
        ("Empty string", ""),
        ("Single character", "‡ÆÖ"),
        ("Single word", "‡ÆÖ‡Æµ‡Æ©‡Øç"),
        ("Only spaces", "   "),
        ("Mixed with punctuation", "‡ÆÖ‡Æµ‡Æ©‡Øç! ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ?"),
        ("Numbers", "123 456"),
        ("English text", "Hello world"),
        ("Code-mixed", "‡Æ®‡Ææ‡Æ©‡Øç Hello ‡Æ™‡Æ£‡Øç‡Æ£‡Æø‡Æü‡Øç‡Æü‡Øá‡Æ©‡Øç"),
    ]
    
    for description, text in edge_cases:
        print(f"\n{description}: '{text}'")
        try:
            marked = sandhi_mark(text, lang="ta")
            tokens = sandhi_split(text, lang="ta")
            token_texts = [token for token, _ in tokens]
            
            print(f"  Marked: '{marked}'")
            print(f"  Tokens: {token_texts}")
            print(f"  Status: ‚úÖ Success")
            
        except Exception as e:
            print(f"  Status: ‚ùå Error - {e}")
    
    return True

def main():
    """Main function to run all analyses."""
    print("Starting Sandhi Rules Analysis...")
    
    # Run all analysis functions
    success = True
    
    success &= analyze_sandhi_rules()
    success &= test_sandhi_rule_categories()
    success &= test_token_generation_process()
    success &= test_reverse_sandhi_verification()
    success &= analyze_rule_application_order()
    success &= test_edge_cases()
    
    print("\n" + "=" * 80)
    print("ANALYSIS COMPLETE")
    print("=" * 80)
    
    if success:
        print("‚úÖ All sandhi rule analyses completed successfully")
        print("\nSUMMARY:")
        print("- Token generation follows Tamil sandhi rules correctly")
        print("- Reverse sandhi implementation is working properly")
        print("- All rule categories are properly applied")
        print("- Edge cases are handled appropriately")
        print("\nCONCLUSION: Sandhi-aware tokenization is properly implemented!")
    else:
        print("‚ùå Some analyses failed - check error messages above")

if __name__ == "__main__":
    main()
